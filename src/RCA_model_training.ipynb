{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when using google collab\n",
    "!git clone https://github.com/Mouret-Orfeu/RCA_LLM_project.git\n",
    "%cd RCA_LLM_project\n",
    "\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e .\n",
    "\n",
    "# restart runtime, so that the environment changes are applied\n",
    "# it raises an error \"session crashed for unknown reason\" but it is expected \n",
    "import os, sys\n",
    "os.kill(os.getpid(), 9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# for locale execution\n",
    "#os.chdir('/home/orfeu/Documents/documents/info_perso/RCA_LLM_project')\n",
    "\n",
    "# for collab usage\n",
    "%cd /content/RCA_LLM_project\n",
    "\n",
    "# Sanity check\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rca_llm.utils import set_seed\n",
    "from rca_llm.RCADataset import RCADataset\n",
    "from rca_llm.trainer import Trainer\n",
    "from rca_llm.HFModelAdapter import HFModelAdapter\n",
    "set_seed(3407)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Hugging face to acces LLama model\n",
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "from huggingface_hub import login\n",
    "login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model you want to use\n",
    "model_type_1 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_type_2 = \"...\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model you want to use\n",
    "user_input = input(\"Enter 1 for meta-llama/Llama-3.2-1B-Instruct \\nor enter 2 for the second model\")\n",
    "if user_input == \"1\":\n",
    "    model_type = model_type_1\n",
    "elif user_input == \"2\":\n",
    "    model_type = model_type_2\n",
    "else:\n",
    "    print(\"Invalid input. Please enter 1 or 2.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type, token=True)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(model_type, token=True, torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
    "model = HFModelAdapter(hf_model, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an example instance of the dataset\n",
    "df = pd.read_csv('./data/itsm_tickets_meaningful_200_utf8.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "# Build disjoint train/test with a shared seed\n",
    "split_seed = 3407\n",
    "train_dataset = RCADataset(df, 'train', tokenizer, seed=split_seed)\n",
    "test_dataset = RCADataset(df, 'test', tokenizer, seed=split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([50256, 50256, 50256,  ...,   415,    13, 50256])\n",
      "Labels: tensor([ -100,  -100,  -100,  ...,   415,    13, 50256])\n",
      "Decoded Input:\n",
      "description du ticket itsm: Bonjour, je ne peux pas utiliser correctement mon audio. Il ne fonctionne pas pendant les appels Teams surtout quand je tente d'envoyer un e-mail. Ce souci est apparu ce matin. Merci pour votre aide.\n",
      "Réponse de l'équipe IT pour la résolution du ticket: Merci pour votre signalement. Le problème était lié à un paramétrage réseau incorrect. Nous avons redémarré le service concerné. Cela devrait être résolu maintenant.\n",
      "\n",
      "Decoded Labels:\n",
      "mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmMerci pour votre signalement. Le problème était lié à un paramétrage réseau incorrect. Nous avons redémarré le service concerné. Cela devrait être résolu maintenant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick check of dataset encoding/decoding process\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "# token ids\n",
    "print(\"Input IDs:\", x)\n",
    "print(\"Labels:\", y)\n",
    "\n",
    "# decoded text\n",
    "print(f\"Decoded Input:\\n{tokenizer.decode(x, skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# For y, I replace all masked tokens (id = -100) by the letter m\n",
    "y = [token if token != -100 else tokenizer.convert_tokens_to_ids('m') for token in y]\n",
    "print(f\"Decoded Labels:\\n{tokenizer.decode(y, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.max_iters = 1000\n",
    "train_config.batch_size = 2\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 0.00327\n",
      "iter_dt 26.96ms; iter 100: train loss 0.02028\n",
      "iter_dt 25.73ms; iter 200: train loss 0.02236\n",
      "iter_dt 33.71ms; iter 300: train loss 0.01246\n",
      "iter_dt 27.47ms; iter 400: train loss 0.02870\n",
      "iter_dt 28.32ms; iter 500: train loss 0.00278\n",
      "iter_dt 45.71ms; iter 600: train loss 0.03206\n",
      "iter_dt 26.38ms; iter 700: train loss 0.00522\n",
      "iter_dt 28.91ms; iter 800: train loss 0.00695\n",
      "iter_dt 27.86ms; iter 900: train loss 0.00521\n",
      "iter_dt 27.65ms; iter 1000: train loss 0.00120\n",
      "iter_dt 40.41ms; iter 1100: train loss 0.01663\n",
      "iter_dt 27.01ms; iter 1200: train loss 0.01429\n",
      "iter_dt 28.49ms; iter 1300: train loss 0.00135\n",
      "iter_dt 29.14ms; iter 1400: train loss 0.01628\n",
      "iter_dt 26.49ms; iter 1500: train loss 0.01357\n",
      "iter_dt 26.22ms; iter 1600: train loss 0.00091\n",
      "iter_dt 27.38ms; iter 1700: train loss 0.03434\n",
      "iter_dt 26.37ms; iter 1800: train loss 0.00266\n",
      "iter_dt 27.40ms; iter 1900: train loss 0.00617\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "        \n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to verify the model generates sensible answers\n",
    "def show_prediction_for_row(\n",
    "        i, \n",
    "        df, \n",
    "        model, \n",
    "        device, \n",
    "        tokenizer, \n",
    "        train_dataset=None,\n",
    "        max_new_tokens=200, \n",
    "        do_sample=False, \n",
    "        temperature=1.0, \n",
    "        top_k=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Prints the question, ground truth answer, and the model's generated answer for row i.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull the raw texts\n",
    "    question = str(df.loc[i, 'ticket_description'])\n",
    "    ground_truth = str(df.loc[i, 'ticket_resolution'])\n",
    "\n",
    "    # Reuse the same prompt format as your dataset\n",
    "    if train_dataset is not None and hasattr(train_dataset, \"prompt_description_addition\") and hasattr(train_dataset, \"prompt_resolution_addition\"):\n",
    "        prompt_prefix = train_dataset.prompt_description_addition\n",
    "        between_prefix = train_dataset.prompt_resolution_addition\n",
    "    else:\n",
    "        # Fallbacks in case you didn't pass the dataset (keep consistent with your training)\n",
    "        prompt_prefix = \"description du ticket itsm: \"\n",
    "        between_prefix = \" Réponse de l'équipe IT pour la résolution du ticket: \"\n",
    "\n",
    "    prompt = f\"{prompt_prefix}{question}{between_prefix}\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate_from_prompt(\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            return_new_text_only=True,      # only the continuation (answer)\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "    print(f\"Row: {i}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"QUESTION:\")\n",
    "    print(question)\n",
    "    print(\"\\nGROUND TRUTH ANSWER:\")\n",
    "    print(ground_truth)\n",
    "    print(\"\\nMODEL GENERATION:\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Example usage (adjust i as you like):\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "i = 0\n",
    "show_prediction_for_row(i, df, model, device, tokenizer, train_dataset=train_dataset,\n",
    "                        max_new_tokens=300, do_sample=False)\n",
    "\n",
    "#Or evaluate a few random rows:\n",
    "# import random\n",
    "# for i in random.sample(range(len(df)), k=5):\n",
    "#     show_prediction_for_row(i, df, model, device, tokenizer, train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re, string\n",
    "\n",
    "def eval_split(\n",
    "    trainer,\n",
    "    split='test',\n",
    "    max_examples=200, # examples on wich generate an answer and compute the metrics, 200 is actually all the dataset\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    print_examples=1,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluate a split on:\n",
    "      - perplexity over answer bytes (for labels != -100)\n",
    "      - QA metrics: Exact Match (EM) and token-level F1\n",
    "      - Generation metric: ROUGE-L (F1)\n",
    "\n",
    "    Returns: (metrics_dict, examples)\n",
    "      metrics_dict = { 'byte_perplexity', 'bits_per_byte', 'exact_match', 'f1', 'rougeL_f1', ... }\n",
    "      examples = list of (question, reference_answer, generated_answer)\n",
    "    \"\"\"\n",
    "\n",
    "    model = trainer.model\n",
    "    device = trainer.device\n",
    "    dataset = {'train': train_dataset, 'test': test_dataset}[split]\n",
    "    df = dataset.df\n",
    "    pad_id = int(model.hf_model.config.pad_token_id)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # BPB (bits_per_byte) over the split\n",
    "    total_nll, total_tokens = 0.0, 0\n",
    "    loader = DataLoader(dataset, batch_size=trainer.config.batch_size, num_workers=trainer.config.num_workers, drop_last=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            tokens = (y != -100).sum().item()\n",
    "            if tokens > 0 and loss is not None:\n",
    "                # loss.item() is the average negative log-likelihood per included token in the batch\n",
    "                # multiply by number of tokens to get total negative log-likelihood for this batch\n",
    "                total_nll += loss.item() * tokens\n",
    "                \n",
    "                # count bytes in the supervised span only\n",
    "                # for each sample, take the positions where y != -100 and x != pad\n",
    "                for i in range(x.size(0)):\n",
    "                    mask = (y[i] != -100) & (x[i] != pad_id)\n",
    "                    if mask.any():\n",
    "                        ids = x[i][mask].tolist()\n",
    "                        txt = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "                        total_bytes += len(txt.encode(\"utf-8\"))\n",
    "\n",
    "    num_bytes = max(total_bytes, 1)\n",
    "    bpb = (total_nll / num_bytes) / math.log(2.0)\n",
    "    byte_perplexity = 2 ** bpb\n",
    "\n",
    "    # standardize the text (lowercase, removes punctuation, removes extra whitespace)\n",
    "    def normalize_text(s):\n",
    "        if s is None:\n",
    "            return ''\n",
    "        s = s.strip().lower()\n",
    "        s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "        return s\n",
    "\n",
    "    def f1_score(prediction, ground_truth):\n",
    "        pred_tokens = normalize_text(prediction).split()\n",
    "        gt_tokens = normalize_text(ground_truth).split()\n",
    "        if len(pred_tokens) == 0 and len(gt_tokens) == 0:\n",
    "            return 1.0\n",
    "        # count overlaps (bag-of-words)\n",
    "        from collections import Counter\n",
    "        pred_counts = Counter(pred_tokens)\n",
    "        gt_counts = Counter(gt_tokens)\n",
    "        overlap = sum((pred_counts & gt_counts).values())\n",
    "        if overlap == 0:\n",
    "            return 0.0\n",
    "        precision = overlap / max(len(pred_tokens), 1)\n",
    "        recall = overlap / max(len(gt_tokens), 1)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    def exact_match(prediction, ground_truth):\n",
    "        return 1.0 if normalize_text(prediction) == normalize_text(ground_truth) else 0.0\n",
    "\n",
    "    # Dynamic programming algorithm to find the length of the Longest Common Subsequence (LCS)\n",
    "    def lcs(x, y):\n",
    "        m, n = len(x), len(y)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(m):\n",
    "            xi = x[i]\n",
    "            dpi = dp[i]\n",
    "            dpi1 = dp[i+1]\n",
    "            for j in range(n):\n",
    "                if xi == y[j]:\n",
    "                    dpi1[j+1] = dpi[j] + 1\n",
    "                else:\n",
    "                    dpi1[j+1] = dpi1[j] if dpi1[j] >= dp[i][j+1] else dp[i][j+1]\n",
    "        return dp[m][n]\n",
    "\n",
    "    def rougeL_f1(prediction, ground_truth):\n",
    "        pred_tokens = normalize_text(prediction).split()\n",
    "        gt_tokens = normalize_text(ground_truth).split()\n",
    "        if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "            return 0.0\n",
    "        # length of the Longest Common Subsequence (LCS) \n",
    "        # (orderded well predicted tokens, not necessarily consecutive)\n",
    "        lcs_len = lcs(pred_tokens, gt_tokens)\n",
    "        prec = lcs_len / len(pred_tokens)\n",
    "        rec = lcs_len / len(gt_tokens)\n",
    "        if prec + rec == 0:\n",
    "            return 0.0\n",
    "        return (2 * prec * rec) / (prec + rec)\n",
    "\n",
    "    # Generation loop for QA metrics\n",
    "    total_exact_match, total_f1, total_rougeL = 0.0, 0.0, 0.0\n",
    "    num_examples = min(max_examples, len(dataset))\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(3407)\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:num_examples]\n",
    "\n",
    "    examples = []  # (question, reference, generated)\n",
    "    with torch.no_grad():\n",
    "        for i_local in indices:\n",
    "            row_idx = int(dataset.ixes[i_local])\n",
    "            question = str(df.loc[row_idx, 'ticket_description'])\n",
    "            reference = str(df.loc[row_idx, 'ticket_resolution'])\n",
    "            prompt = dataset.prompt_description_addition + question + dataset.prompt_resolution_addition\n",
    "\n",
    "            generated = model.generate_from_prompt(\n",
    "                prompt=prompt,\n",
    "                device=device,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                return_new_text_only=True,\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "\n",
    "            examples.append((question, reference, generated))\n",
    "            total_exact_match += exact_match(generated, reference)\n",
    "            total_f1 += f1_score(generated, reference)\n",
    "            total_rougeL += rougeL_f1(generated, reference)\n",
    "\n",
    "    qa_em = total_exact_match / max(num_examples, 1)\n",
    "    qa_f1 = total_f1 / max(num_examples, 1)\n",
    "    rougeL = total_rougeL / max(num_examples, 1)\n",
    "\n",
    "    results = {\n",
    "        'split': split,\n",
    "        'examples_evaluated': int(num_examples),\n",
    "        'byte_perplexity': float(byte_perplexity) if total_bytes > 0 else None,\n",
    "        'bits_per_byte': float(bpb) if total_bytes > 0 else None,\n",
    "        'exact_match': float(qa_em),\n",
    "        'f1': float(qa_f1),\n",
    "        'rougeL_f1': float(rougeL),\n",
    "    }\n",
    "\n",
    "    if print_examples > 0:\n",
    "        for k, (q, ref, pred) in enumerate(examples[:print_examples]):\n",
    "            print(f'[#{k}] QUESTION: {q}')\n",
    "            print(f'     REF    : {ref}')\n",
    "            print(f'     PRED   : {pred}')\n",
    "            print('-' * 60)\n",
    "\n",
    "    print(\n",
    "        f\"Eval {split}: PPL={results['perplexity']:.3f} | EM={qa_em*100:.2f}% | F1={qa_f1*100:.2f}% | ROUGE-L={rougeL*100:.2f}% | examples={num_examples}\"\n",
    "    )\n",
    "    return results, examples\n",
    "\n",
    "# Example: evaluate both splits\n",
    "with torch.no_grad():\n",
    "    train_metrics, _ = eval_split(trainer, 'train', max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)\n",
    "    test_metrics, _  = eval_split(trainer, 'test',  max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence  : [[0, 0, 2, 1, 0, 1]]\n",
      "predicted sorted: [[0, 0, 0, 1, 1, 2]]\n",
      "gt sort         : [0, 0, 0, 1, 1, 2]\n",
      "matches         : True\n"
     ]
    }
   ],
   "source": [
    "# let's run a random given sequence through the model as well\n",
    "n = train_dataset.length # naugy direct access shrug\n",
    "inp = torch.tensor([[0, 0, 2, 1, 0, 1]], dtype=torch.long).to(trainer.device)\n",
    "assert inp[0].nelement() == n\n",
    "with torch.no_grad():\n",
    "    cat = model.generate(inp, n, do_sample=False)\n",
    "sol = torch.sort(inp[0])[0]\n",
    "sol_candidate = cat[:, n:]\n",
    "print('input sequence  :', inp.tolist())\n",
    "print('predicted sorted:', sol_candidate.tolist())\n",
    "print('gt sort         :', sol.tolist())\n",
    "print('matches         :', bool((sol == sol_candidate).all()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
